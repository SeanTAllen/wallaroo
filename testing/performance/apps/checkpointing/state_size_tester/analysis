#!/usr/bin/env python3

import argparse
import json
from matplotlib import (
            pyplot as plt,
            ticker,
            gridspec,
            rc)
import numpy as np
import pandas as pd

import file_parser


parser = argparse.ArgumentParser("State Size Test Analysis")
parser.add_argument("-o", "--output-prefix", type=str, dest='prefix',
                    help="The path prefix for saving the post-processed data.")

# Use groups for mutually exclusive features
mex = parser.add_mutually_exclusive_group(required=True)

mex.add_argument("-d", "--application-data", type=str,
                    help="The path to the application output file.")
mex.add_argument("-p", "--processed-data", type=str,
                    help="The path to the processed data file.")
parser.add_argument("-c", "--checkpoints", type=str, default=None,
                    help="The path to the checkpoints stats file.")


args = parser.parse_args()

paths = {'post-processed': args.prefix + '_processed.txt',
         'png': args.prefix + '_performance.png',
         'svg': args.prefix + '_performance.svg',
         'stats': args.prefix + '_stats.txt'}

if args.application_data:
    perf = file_parser.load_output_file(args.application_data)
    if args.checkpoints:
        checkpoints = file_parser.load_checkpoints_file(args.checkpoints)
        # assume only one worker for now
        checkpoints = checkpoints.popitem()[1]
    else:
        checkpoints = {}


    # Use perf data for the time boundaries
    min_ts = min(perf.keys())
    # create a panads dataframe
    data = {}
    for k in list(perf.keys()):
        key = k - min_ts
        data[key] = perf.pop(k)
        data[key].update(checkpoints.get(k, {}))
        pct = data[key].pop('percentiles')
        for p, v in sorted(pct, key=lambda x: x[0]):
            data[key]['p{}'.format(p)] = v

    df = pd.DataFrame.from_dict(data, orient='index')

    with open(paths['post-processed'], 'wt') as f:
        df.to_json(f)
        print("Post-processed data written to {!r}".format(paths['post-processed']))

elif args.processed_data:
    print("Loading processed data from {!r}".format(args.processed_data))
    with open(args.processed_data, 'rt') as f:
        d = json.loads(f.read())
        for col in d:
            for k in list(d[col].keys()):
                d[col][int(k)] = d[col].pop(k)
        df = pd.DataFrame.from_dict(d)

print(df[['count', 'size', 'id', 'p50', 'p95', 'p99', 'p99.9', 'p99.99', 'p100']])
# Get stats
stats = df[['count', 'p50', 'p95', 'p99', 'p99.9', 'p99.99', 'p100']].describe()
# print stats
print(stats)
# save stats
with open(paths['stats'], 'wt') as f:
    stats.to_json(f)
print("Stats data written to {!r}".format(paths['stats']))


rc('font', size=12)

fig = plt.figure(figsize=(12, 18), dpi=120, facecolor='w', edgecolor='k')
fig.suptitle(args.prefix)
grid = plt.GridSpec(4, 3) # 4 rows, 3 cols

# First pane: Throughput vs. Time, full width
pane1 = plt.subplot(grid[0,:])
pane1.set_title('Throughput')
pane1.set_xlabel('Time (s)')
# rescale Y axis
ticks_y = ticker.FuncFormatter(lambda y, pos: '{0:g}'.format(y/1000))
pane1.yaxis.set_major_formatter(ticks_y)
pane1.set_ylabel('K req/sec')
pane1.xaxis.set_minor_locator(ticker.MultipleLocator(10))
pane1.xaxis.set_major_locator(ticker.MultipleLocator(50))
pane1.set_ylim(0,df['count'].max())
df.plot(y='count', ax=pane1, legend=False)

## Panes 2,3: Latency vs. Time, 3 per row
row_2, row_3= ['p50', 'p95', 'p99'], ['p99.9', 'p99.99', 'p100']
ylim = df[row_2+row_3].max().max()
for i, r in enumerate([row_2, row_3]):
    for j, p in enumerate(r):
        subpane = plt.subplot(grid[i+1,j])
        subpane.set_title(p)
        subpane.set_xlabel('Time (s)')
        subpane.set_ylabel('Latency (ms)')
        subpane.set_ylim(0, ylim)
        ticks_y = ticker.FuncFormatter(lambda y, pos: '{0:g}'.format(y*1000))
        subpane.yaxis.set_major_formatter(ticks_y)
        df.plot(y=p, ax=subpane, legend=False)

# Stats as a table plot in bottom pane
bottom = plt.subplot(grid[3,:])
# Set pandas display precision to 2 decimals
# get stats
tf = df[['count', 'p50', 'p95', 'p99', 'p99.9', 'p99.99', 'p100']].describe()
# convert the percentile columns to ms
for col in tf.columns:
    if col.startswith('p'):
        tf[col] *= 1000
# convert the counts column to 1k/
tf['count'] /=1000.0
# Convert everything to 2 decimal precision
tf = tf.applymap("{0:.2f}".format)
# Replace count row with units
row = pd.Series({c: 'ms' if c.startswith('p') else 'KReq/s'
                     for c in tf.columns}, name='unit')
tf.iloc[0] = row
tf.rename(index={'count':'unit'}, inplace=True)
# add data rows
cell_text = []
for row in range(len(tf)):
    cell_text.append(tf.iloc[row])
table = bottom.table(cellText=cell_text, colLabels=tf.columns, loc='center',
                     rowLabels=tf.index)

bottom.axis('off')


plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # otherwise the right y-label is slightly clipped

#plt.show()
plt.savefig(paths['png'], format='png')
print("Figure saved to {!r}".format(paths['png']))
